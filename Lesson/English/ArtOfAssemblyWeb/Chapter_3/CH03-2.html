<html>
<!-- Generated by Harlequin WebMaker 2.2.6 (30-Apr-1996)Macintosh Common Lisp Version 3.0kp2p2 [AppGen 3.0b1kp2p2] -->


<!-- Mirrored from www.arl.wustl.edu/~lockwood/class/cs306/books/artofasm/Chapter_3/CH03-2.html by HTTrack Website Copier/3.x [XR&CO'2008], Fri, 05 Dec 2008 15:25:02 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8"><!-- /Added by HTTrack -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta http-equiv="pragma" content="no-cache">
<title>CHAPTER THREE: SYSTEM ORGANIZATION (Part 2)</title>
</head>

<body topmargin="10" stylesrc="../toc.html" bgcolor="#FFFFFF" text="#000000" link="#008000" vlink="#000000">
<div align="center"><center>

<table border="0" width="100%" cellspacing="0" cellpadding="0">
  <tr>
    <td width="100%" colspan="3"><p align="right"><a name="top"></a><font size="1" face="Arial">The Art of<br>
    </font><font face="Arial Black" size="1">ASSEMBLY LANGUAGE PROGRAMMING</font></td>
  </tr>
  <tr>
    <td width="100%" valign="middle" align="center" nowrap bgcolor="#000000" height="1" colspan="3"><a NAME="HEADING2"></a></td>
  </tr>
  <tr>
    <td width="34%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><p align="left"><a href="CH03-1.html"><img src="../images/WB00823_.GIF" align="absmiddle" border="0" WIDTH="12" HEIGHT="24"></a><font face="Arial" size="2"><strong> <a href="CH03-1.html">Chapter Three</a> (Part 1)</strong></font></td>
    <td width="33%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><a href="../toc.html"><font face="Arial" size="2"><strong>Table of Content</strong></font></a></td>
    <td width="33%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><p align="right"><font face="Arial" size="2"><strong><a href="CH03-3.html">Chapter Three</a> (Part 3) </strong></font><a href="CH03-3.html"><img src="../images/WB00827_.GIF" align="absmiddle" border="0" WIDTH="12" HEIGHT="24"></a></td>
  </tr>
</table>
</center></div><div align="center"><center>

<table border="0" width="100%" cellspacing="0" cellpadding="0">
  <tr>
    <td width="100%" bgcolor="#FFFFFF" nowrap height="10"><a NAME="HEADING2-0"></a></td>
  </tr>
  <tr>
    <td width="100%" bgcolor="#F0F0F0"><font face="Arial" size="4"><strong>CHAPTER THREE:<br>
    SYSTEM ORGANIZATION (Part 2)</strong></font></td>
  </tr>
  <tr>
    <td width="100%" nowrap height="10"></td>
  </tr>
  <tr>
    <td width="100%"><font face="Arial" size="2"><a HREF="#HEADING2-1"><b>3.2 </b>- System
    Timing</a> <br>
    <a HREF="#HEADING2-9"><b>3.2.1 </b>- The System Clock</a> <br>
    <a HREF="#HEADING2-17"><b>3.2.2 </b>- Memory Access and the System Clock</a> <br>
    <a HREF="#HEADING2-30"><b>3.2.3 </b>- Wait States</a> <br>
    <a HREF="#HEADING2-45"><b>3.2.4 </b>- Cache Memory</a> </font></td>
  </tr>
  <tr>
    <td width="100%" nowrap height="20"></td>
  </tr>
  <tr>
    <td width="100%"><strong><font face="Arial" size="3"><a NAME="HEADING2-1"></a>3.2 System
    Timing</font></strong></td>
  </tr>
  <tr>
    <td width="100%" nowrap bgcolor="#000000" height="1"></td>
  </tr>
</table>
</center></div>

<p><font face="Arial" size="2">Although modern computers are quite fast and getting faster
all the time, they still require a finite amount of time to accomplish even the smallest
tasks. On Von Neumann machines, like the 80x86, most operations are <i>serialized</i>.
This means that the computer executes commands in a prescribed order. It wouldn't do, for
example, to execute the statement <code>I:=I*5+2</code>; before I<code>:=J</code>; in the
following sequence: </font></p>

<pre><font face="Courier New" size="2">	I := J;
	I := I * 5 + 2;</font></pre>

<p><font face="Arial" size="2">Clearly we need some way to control which statement
executes first and which executes second. </font></p>

<p><font face="Arial" size="2">Of course, on real computer systems, operations do not
occur instantaneously. Moving a copy of <code>J </code>into <code>I</code> takes a certain
amount of time. Likewise, multiplying <code>I</code> by five and then adding two and
storing the result back into <code>I </code>takes time. As you might expect, the second
Pascal statement above takes quite a bit longer to execute than the first. For those
interested in writing fast software, a natural question to ask is, &quot;How does the
processor execute statements, and how do we measure how long they take to execute?&quot; </font></p>

<p><font face="Arial" size="2">The CPU is a very complex piece of circuitry. Without going
into too many details, let us just say that operations inside the CPU must be very
carefully coordinated or the CPU will produce erroneous results. To ensure that all
operations occur at just the right moment, the 80x86 CPUs use an alternating signal called
the <i>system clock</i>. </font></p>

<p><strong><font face="Arial" size="3"><a NAME="HEADING2-9"></a>3.2.1 The System Clock</font></strong></p>

<p><font face="Arial" size="2">At the most basic level, the <i>system clock </i>handles
all synchronization within a computer system. The system clock is an electrical signal on
the control bus which alternates between zero and one at a periodic rate:</font></p>

<p align="center"><img SRC="images/ch03a9.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a9.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="366" HEIGHT="102"> </p>

<p><font face="Arial" size="2">CPUs are a good example of a complex synchronous logic
system (see the previous chapter). The system clock gates many of the logic gates that
make up the CPU allowing them to operate in a synchronized fashion.</font></p>

<p><font face="Arial" size="2">The frequency with which the system clock alternates
between zero and one is the <i>system clock frequency</i>. The time it takes for the
system clock to switch from zero to one and back to zero is the <i>clock period. </i>One
full period is also called a <i>clock cycle</i>. On most modern systems, the system clock
switches between zero and one at rates exceeding several million times per second. The
clock frequency is simply the number of clock cycles which occur each second. A typical
80486 chip runs at speeds of 66million cycles per second. &quot;Hertz&quot; (Hz) is the
technical term meaning one cycle per second. Therefore, the aforementioned 80486 chip runs
at 66 million hertz, or 66 megahertz (MHz). Typical frequencies for 80x86 parts range from
5 MHz up to 200 MHz and beyond. Note that one clock period (the amount of time for one
complete clock cycle) is the reciprocal of the clock frequency. For example, a 1 MHz clock
would have a clock period of one microsecond (1/1,000,000<em>th</em> of a second).
Likewise, a 10 MHz clock would have a clock period of 100 nanoseconds (100 billionths of a
second). A CPU running at 50 MHz would have a clock period of 20 nanoseconds. Note that we
usually express clock periods in millionths or billionths of a second.</font></p>

<p><font face="Arial" size="2">To ensure synchronization, most CPUs start an operation on
either the <i>falling edge </i>(when the clock goes from one to zero) or the<i> rising
edge</i> (when the clock goes from zero to one). The system clock spends most of its time
at either zero or one and very little time switching between the two. Therefore clock edge
is the perfect synchronization point.</font></p>

<p><font face="Arial" size="2">Since all CPU operations are synchronized around the clock,
the CPU cannot perform tasks any faster than the clock. However, just because a CPU is
running at some clock frequency doesn't mean that it is executing that many operations
each second. Many operations take multiple clock cycles to complete so the CPU often
performs operations at a significantly lower rate.</font></p>

<p align="left"><font face="Arial" size="3"><strong><a NAME="HEADING2-17"></a> 3.2.2
Memory Access and the System Clock </strong></font></p>

<p align="left"><font face="Arial" size="2">Memory access is probably the most common CPU
activity. Memory access is definitely an operation synchronized around the system clock.
That is, reading a value from memory or writing a value to memory occurs no more often
than once every clock cycle. Indeed, on many 80x86 processors, it takes several clock
cycles to access a memory location. The memory access time is the number of clock cycles
the system requires to access a memory location; this is an important value since longer
memory access times result in lower performance.</font></p>

<p align="left"><font face="Arial" size="2">Different 80x86 processors have different
memory access times ranging from one to four clock cycles. For example, the 8088 and 8086
CPUs require <i>four </i>clock cycles to access memory; the 80486 requires only one.
Therefore, the 80486 will execute programs which access memory faster than an 8086, even
when running at the same clock frequency.</font></p>

<p align="left"><font face="Arial" size="2">Memory access time is the amount of time
between a memory operation request (read or write) and the time the memory operation
completes. On a 5 MHz 8088/8086 CPU the memory access time is roughly 800 ns
(nanoseconds). On a 50 MHz 80486, the memory access time is slightly less than 20 ns. Note
that the memory access time for the 80486 is 40 times faster than the 8088/8086. This is
because the 80486's clock frequency is ten times faster and it uses one-fourth the clock
cycles to access memory.</font></p>

<p align="left"><font face="Arial" size="2">When reading from memory, the memory access
time is the amount of time from the point that the CPU places an address on the address
bus and the CPU takes the data off the data bus. On an 80486 CPU with a one cycle memory
access time, a read looks something like shown below:</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a10.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a10.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="419" HEIGHT="185"> </font></p>

<p><font face="Arial" size="2">Writing data to memory is similar to:</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a11.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a11.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="414" HEIGHT="93"></font></p>

<p><font face="Arial" size="2">Note that the CPU doesn't wait for memory. The access time
is specified by the clock frequency. If the memory subsystem doesn't work fast enough, the
CPU will read garbage data on a memory read operation and will not properly store the data
on a memory write operation. This will surely cause the system to fail.</font></p>

<p><font face="Arial" size="2">Memory devices have various ratings, but the two major ones
are capacity and speed (access time). Typical dynamic RAM (random access memory) devices
have capacities of four (or more) megabytes and speeds of 50-100 ns. You can buy bigger or
faster devices, but they are much more expensive. A typical 33 MHz 80486 system uses 70 ns
memory devices.</font></p>

<p><font face="Arial" size="2">Wait just a second here! At 33 MHz the clock period is
roughly 33 ns. How can a system designer get away with using 70 ns memory? The answer is <i>wait
states</i>.</font></p>

<p><strong><font face="Arial" size="3"><a NAME="HEADING2-30"></a>3.2.3 Wait States</font></strong></p>

<p><font face="Arial" size="2">A wait state is nothing more than an extra clock cycle to
give some device time to complete an operation. For example, a 50 MHz 80486 system has a
20 ns clock period. This implies that you need 20 ns memory. In fact, the situation is
worse than this. In most computer systems there is additional circuitry between the CPU
and memory: decoding and buffering logic. This additional circuitry introduces additional
delays into the system:</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a12.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a12.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="222" HEIGHT="196"> </font></p>

<p><font face="Arial" size="2">In this diagram, the system loses 10ns to buffering and
decoding. So if the CPU needs the data back in 20 ns, the memory must respond in less than
10 ns.</font></p>

<p><font face="Arial" size="2">You can actually buy 10ns memory. However, it is very
expensive, bulky, consumes a lot of power, and generates a lot of heat. These are bad
attributes. Supercomputers use this type of memory. However, supercomputers also cost
millions of dollars, take up entire rooms, require special cooling, and have giant power
supplies. Not the kind of stuff you want sitting on your desk.</font></p>

<p><font face="Arial" size="2">If cost-effective memory won't work with a fast processor,
how do companies manage to sell fast PCs? One part of the answer is the wait state. For
example, if you have a 20 MHz processor with a memory cycle time of 50 ns and you lose 10
ns to buffering and decoding, you'll need 40 ns memory. What if you can only afford 80 ns
memory in a 20 MHz system? Adding a wait state to extend the memory cycle to 100 ns (two
clock cycles) will solve this problem. Subtracting 10ns for the decoding and buffering
leaves 90 ns. Therefore, 80 ns memory will respond well before the CPU requires the data.</font></p>

<p><font face="Arial" size="2">Almost every general purpose CPU in existence provides a
signal on the control bus to allow the insertion of wait states. Generally, the decoding
circuitry asserts this line to delay one additional clock period, if necessary. This gives
the memory sufficient access time, and the system works properly</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a13.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a13.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="419" HEIGHT="208"> </font></p>

<p><font face="Arial" size="2">Sometimes a single wait state is not sufficient. Consider
the 80486 running at 50 MHz. The normal memory cycle time is less than 20 ns. Therefore,
less than 10 ns are available after subtracting decoding and buffering time. If you are
using 60 ns memory in the system, adding a single wait state will not do the trick. Each
wait state gives you 20 ns, so with a single wait state you would need 30 ns memory. To
work with 60 ns memory you would need to add <i>three </i>wait states (zero wait states =
10 ns, one wait state = 30 ns, two wait states = 50 ns, and three wait states = 70 ns).</font></p>

<p><font face="Arial" size="2">Needless to say, from the system performance point of view,
wait states are <i>not </i>a good thing. While the CPU is waiting for data from memory it
cannot operate on that data. Adding a single wait state to a memory cycle on an 80486 CPU <i>doubles</i>
the amount of time required to access the data. This, in turn, <i>halves</i> the speed of
the memory access. Running with a wait state on every memory access is almost like cutting
the processor clock frequency in half. You're going to get a lot less work done in the
same amount of time.</font></p>

<p><font face="Arial" size="2">You've probably seen the ads. &quot;80386DX, 33 MHz, 8
megabytes 0 wait state RAM... only $1,000!&quot; If you look closely at the specs you'll
notice that the manufacturer is using 80 ns memory. How can they build systems which run
at 33 MHz and have zero wait states? Easy. They lie. </font></p>

<p><font face="Arial" size="2">There is no way an 80386 can run at 33 MHz, executing an
arbitrary program, without ever inserting a wait state. It is flat out impossible.
However, it is quite possible to design a memory subsystem which <i>under certain,
special, circumstances</i> manages to operate without wait states part of the time. Most
marketing types figure if their system <i>ever </i>operates at zero wait states, they can
make that claim in their literature. Indeed, most marketing types have no idea what a wait
state is other than it's bad and having zero wait states is something to brag about.</font></p>

<p><font face="Arial" size="2">However, we're not doomed to slow execution because of
added wait states. There are several tricks hardware designers can play to achieve zero
wait states <i>most </i>of the time. The most common of these is the use of <i>cache </i>(pronounced
&quot;cash&quot;) memory.</font></p>

<p><strong><font face="Arial" size="3"><a NAME="HEADING2-45"></a>3.2.4 Cache Memory</font></strong></p>

<p><font face="Arial" size="2">If you look at a typical program (as many researchers
have), you'll discover that it tends to access the same memory locations repeatedly.
Furthermore, you also discover that a program often accesses adjacent memory locations.
The technical names given to this phenomenon are <i>temporal locality of reference </i>and<i>
spatial locality of reference</i>. When exhibiting spatial locality, a program accesses
neighboring memory locations. When displaying temporal locality of reference a program
repeatedly accesses the same memory location during a short time period. Both forms of
locality occur in the following Pascal code segment: </font></p>

<pre><font face="Courier New" size="2">	for i := 0 to 10 do
		A [i] := 0;</font></pre>

<p><font face="Arial" size="2">There are two occurrences each of spatial and temporal
locality of reference within this loop. Let's consider the obvious ones first.</font></p>

<p><font face="Arial" size="2">In the Pascal code above, the program references the
variable <code>i</code> several times. The <code>for</code> loop compares <code>i</code>
against 10 to see if the loop is complete. It also increments <code>i </code>by one at the
bottom of the loop. The assignment statement also uses <code>i</code> as an array index.
This shows temporal locality of reference in action since the CPU accesses <code>i</code>
at three points in a short time period.</font></p>

<p><font face="Arial" size="2">This program also exhibits spatial locality of reference.
The loop itself zeros out the elements of array <code>A</code> by writing a zero to the
first location in <code>A</code>, then to the second location in <code>A</code>, and so
on. Assuming that Pascal stores the elements of <code>A</code> into consecutive memory
locations, each loop iteration accesses adjacent memory locations.</font></p>

<p><font face="Arial" size="2">There is an additional example of temporal and spatial
locality of reference in the Pascal example above, although it is not so obvious. Computer
<i>instructions</i> which tell the system to do the specified task also appear in memory.
These instructions appear sequentially in memory - the spatial locality part. The computer
also executes these instructions repeatedly, once for each loop iteration - the temporal
locality part.</font></p>

<p><font face="Arial" size="2">If you look at the execution profile of a typical program,
you'd discover that the program typically executes less than half the statements.
Generally, a typical program might only use 10-20% of the memory allotted to it. At any
one given time, a one megabyte program might only access four to eight kilobytes of data
and code. So if you paid an outrageous sum of money for expensive zero wait state RAM, you
wouldn't be using most of it at any one given time! Wouldn't it be nice if you could buy a
small amount of fast RAM and dynamically reassign its address(es) as the program executes?</font></p>

<p><font face="Arial" size="2">This is exactly what cache memory does for you. Cache
memory sits between the CPU and main memory. It is a small amount of very fast (zero wait
state) memory. Unlike normal memory, the bytes appearing within a cache do not have fixed
addresses. Instead, cache memory can reassign the address of a data object. This allows
the system to keep recently accessed values in the cache. Addresses which the CPU has
never accessed or hasn't accessed in some time remain in main (slow) memory. Since most
memory accesses are to recently accessed variables (or to locations near a recently
accessed location), the data generally appears in cache memory.</font></p>

<p><font face="Arial" size="2">Cache memory is not perfect. Although a program may spend
considerable time executing code in one place, eventually it will call a procedure or
wander off to some section of code outside cache memory. In such an event the CPU has to
go to main memory to fetch the data. Since main memory is slow, this will require the
insertion of wait states.</font></p>

<p><font face="Arial" size="2">A cache <i>hit </i>occurs whenever the CPU accesses memory
and finds the data in the cache. In such a case the CPU can usually access data with zero
wait states. A cache <i>miss </i>occurs if the CPU accesses memory and the data is not
present in cache. Then the CPU has to read the data from main memory, incurring a
performance loss. To take advantage of locality of reference, the CPU copies data into the
cache whenever it accesses an address not present in the cache. Since it is likely the
system will access that same location shortly, the system will save wait states by having
that data in the cache.</font></p>

<p><font face="Arial" size="2">As described above, cache memory handles the temporal
aspects of memory access, but not the spatial aspects. Caching memory locations<i> when
you access them</i> won't speed up the program if you constantly access consecutive
locations (spatial locality of reference). To solve this problem, most caching systems
read several consecutive bytes from memory when a cache miss occurs. The 80486, for
example, reads 16 bytes at a shot upon a cache miss. If you read 16 bytes, why read them
in blocks rather than as you need them? As it turns out, most memory chips available today
have special modes which let you quickly access several consecutive memory locations on
the chip. The cache exploits this capability to reduce the average number of wait states
needed to access memory.</font></p>

<p><font face="Arial" size="2">If you write a program that randomly accesses memory, using
a cache might actually slow you down. Reading 16 bytes on each cache miss is expensive if
you only access a few bytes in the corresponding cache line. Nonetheless, cache memory
systems work quite well.</font></p>

<p><font face="Arial" size="2">It should come as no surprise that the ratio of cache hits
to misses increases with the size (in bytes) of the cache memory subsystem. The 80486
chip, for example, has 8,192 bytes of on-chip cache. Intel claims to get an 80-95% hit
rate with this cache (meaning 80-95% of the time the CPU finds the data in the cache).
This sounds very impressive. However, if you play around with the numbers a little bit,
you'll discover it's not all <i>that </i>impressive. Suppose we pick the 80% figure. Then
one out of every five memory accesses, on the average, will not be in the cache. If you
have a 50 MHz processor and a 90 ns memory access time, four out of five memory accesses
require only one clock cycle (since they are in the cache) and the fifth will require
about 10 wait states. Altogether, the system will require 15 clock cycles to access five
memory locations, or three clock cycles per access, on the average. That's equivalent to
two wait states added to every memory access. Now do you believe that your machine runs at
zero wait states?</font></p>

<p><font face="Arial" size="2">There are a couple of ways to improve the situation. First,
you can add more cache memory. This improves the cache hit ratio, reducing the number of
wait states. For example, increasing the hit ratio from 80% to 90% lets you access 10
memory locations in 20 cycles. This reduces the average number of wait states per memory
access to one wait state - a substantial improvement. Alas, you can't pull an 80486 chip
apart and solder more cache onto the chip. However, the 80586/Pentium CPU has a
significantly larger cache than the 80486 and operates with fewer wait states. </font></p>

<p><font face="Arial" size="2">Another way to improve performance is to build a <i>two-level
</i>caching system. Many 80486 systems work in this fashion. The first level is the
on-chip 8,192 byte cache. The next level, between the on-chip cache and main memory, is a
secondary cache built on the computer system circuit board:</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a14.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a14.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="303" HEIGHT="155"> </font></p>

<p><font face="Arial" size="2">A typical secondary cache contains anywhere from 32,768
bytes to one megabyte of memory. Common sizes on PC subsystems are 65,536 and 262,144
bytes of cache.</font></p>

<p><font face="Arial" size="2">You might ask, &quot;Why bother with a two-level cache? Why
not use a 262,144 byte cache to begin with?&quot; Well, the secondary cache generally does
not operate at zero wait states. The circuitry to support 262,144 bytes of 10 ns memory
(20 ns total access time) would be <i>very </i>expensive. So most system designers use
slower memory which requires one or two wait states. This is still <i>much </i>faster than
main memory. Combined with the on-chip cache, you can get better performance from the
system.</font></p>

<p><font face="Arial" size="2">Consider the previous example with an 80% hit ratio. If the
secondary cache requires two cycles for each memory access and three cycles for the first
access, then a cache miss on the on-chip cache will require a total of six clock cycles.
All told, the average system performance will be two clocks per memory access. Quite a bit
faster than the three required by the system without the secondary cache. Furthermore, the
secondary cache can update its values in parallel with the CPU. So the number of cache
misses (which affect CPU performance) goes way down.</font></p>

<p><font face="Arial" size="2">You're probably thinking, &quot;So far this all sounds
interesting, but what does it have to do with programming?&quot; Quite a bit, actually. By
writing your program carefully to take advantage of the way the cache memory system works,
you can improve your program's performance. By colocating variables you commonly use
together in the same cache line, you can force the cache system to load these variables as
a group, saving extra wait states on each access.</font></p>

<p><font face="Arial" size="2">If you organize your program so that it tends to execute
the same sequence of instructions repeatedly, it will have a high degree of temporal
locality of reference and will, therefore, execute faster.</font></p>
<div align="center"><center>

<table border="0" width="100%" cellspacing="0" cellpadding="0">
  <tr>
    <td width="100%" valign="middle" align="center" nowrap bgcolor="#000000" height="1" colspan="3"></td>
  </tr>
  <tr>
    <td width="34%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><p align="left"><a href="CH03-1.html"><img src="../images/WB00823_.GIF" align="absmiddle" border="0" WIDTH="12" HEIGHT="24"></a><font face="Arial" size="2"><strong> <a href="CH03-1.html">Chapter Three</a> (Part 1)</strong></font></td>
    <td width="33%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><a href="../toc.html"><font face="Arial" size="2"><strong>Table of Content</strong></font></a></td>
    <td width="33%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><p align="right"><font face="Arial" size="2"><strong><a href="CH03-3.html">Chapter Three</a> (Part 3) </strong></font><a href="CH03-3.html"><img src="../images/WB00827_.GIF" align="absmiddle" border="0" WIDTH="12" HEIGHT="24"></a></td>
  </tr>
</table>
</center></div>

<p align="right"><font face="Arial" size="2"><strong>Chapter Three: System Organization
(Part 2)<br>
26 SEP 1996</strong></font></p>
</body>

<!-- Mirrored from www.arl.wustl.edu/~lockwood/class/cs306/books/artofasm/Chapter_3/CH03-2.html by HTTrack Website Copier/3.x [XR&CO'2008], Fri, 05 Dec 2008 15:25:08 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8"><!-- /Added by HTTrack -->
</html>
