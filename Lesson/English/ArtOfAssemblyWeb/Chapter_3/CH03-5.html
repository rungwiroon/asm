<html>
<!-- Generated by Harlequin WebMaker 2.2.6 (30-Apr-1996)Macintosh Common Lisp Version 3.0kp2p2 [AppGen 3.0b1kp2p2] -->


<!-- Mirrored from www.arl.wustl.edu/~lockwood/class/cs306/books/artofasm/Chapter_3/CH03-5.html by HTTrack Website Copier/3.x [XR&CO'2008], Fri, 05 Dec 2008 15:25:18 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8"><!-- /Added by HTTrack -->
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<meta http-equiv="pragma" content="no-cache">
<title>CHAPTER THREE: SYSTEM ORGANIZATION (Part 5)</title>
</head>

<body topmargin="10" stylesrc="../toc.html" bgcolor="#FFFFFF" text="#000000" link="#008000" vlink="#000000">
<div align="center"><center>

<table border="0" width="100%" cellspacing="0" cellpadding="0">
  <tr>
    <td width="100%" colspan="3"><p align="right"><a name="top"></a><font size="1" face="Arial">The Art of<br>
    </font><font face="Arial Black" size="1">ASSEMBLY LANGUAGE PROGRAMMING</font></td>
  </tr>
  <tr>
    <td width="100%" valign="middle" align="center" nowrap bgcolor="#000000" height="1" colspan="3"><a NAME="HEADING5"></a></td>
  </tr>
  <tr>
    <td width="34%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><p align="left"><a href="CH03-4.html"><img src="../images/WB00823_.GIF" align="absmiddle" border="0" WIDTH="12" HEIGHT="24"></a><font face="Arial" size="2"><strong> <a href="CH03-4.html">Chapter Three</a> (Part 4)</strong></font></td>
    <td width="33%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><a href="../toc.html"><font face="Arial" size="2"><strong>Table of Content</strong></font></a></td>
    <td width="33%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><p align="right"><font face="Arial" size="2"><strong><a href="CH03-6.html">Chapter Three</a> (Part 6) </strong></font><a href="CH03-6.html"><img src="../images/WB00827_.GIF" align="absmiddle" border="0" WIDTH="12" HEIGHT="24"></a></td>
  </tr>
</table>
</center></div><div align="center"><center>

<table border="0" width="100%" cellspacing="0" cellpadding="0">
  <tr>
    <td width="100%" bgcolor="#FFFFFF" nowrap height="10" colspan="2"><a NAME="HEADING5-0"></a></td>
  </tr>
  <tr>
    <td width="100%" bgcolor="#F0F0F0" colspan="2"><font face="Arial" size="4"><strong>CHAPTER
    THREE:<br>
    SYSTEM ORGANIZATION (Part 5)</strong></font></td>
  </tr>
  <tr>
    <td width="100%" nowrap height="10" colspan="2"></td>
  </tr>
  <tr>
    <td width="50%" valign="top"><font face="Arial" size="2"><a HREF="#HEADING5-1"><b>3.3.12 </b>-
    The 8486 Processor</a> <br>
    <a HREF="#HEADING5-4"><b>3.3.12.1 </b>- The 8486 Pipeline</a> <br>
    <a HREF="#HEADING5-23"><b>3.3.12.2 </b>- Stalls in a Pipeline</a> </font></td>
    <td width="50%" valign="top"><font face="Arial" size="2"><a HREF="#HEADING5-36"><b>3.3.12.3
    </b>- Cache, the Prefetch Queue, and the 8486</a> <br>
    <a HREF="#HEADING5-70"><b>3.3.12.4 </b>- Hazards on the 8486</a> <br>
    <a HREF="#HEADING5-95"><b>3.3.13 </b>- The 8686 Processor</a> </font></td>
  </tr>
  <tr>
    <td width="100%" nowrap height="20" colspan="2"></td>
  </tr>
  <tr>
    <td width="100%" colspan="2"><strong><font face="Arial" size="3"><a NAME="HEADING5-1"></a>3.3.12
    The 8486 Processor</font></strong></td>
  </tr>
</table>
</center></div>

<p><font face="Arial" size="2">Executing instructions in parallel using a bus interface
unit and an execution unit is a special case of <i>pipelining</i>. The 8486 incorporates
pipelining to improve performance. With just a few exceptions, we'll see that pipelining
allows us to execute one instruction per clock cycle.</font></p>

<p><font face="Arial" size="2">The advantage of the prefetch queue was that it let the CPU
overlap instruction fetching and decoding with instruction execution. That is, while one
instruction is executing, the BIU is fetching and decoding the next instruction. Assuming
you're willing to add hardware, you can execute almost all operations in parallel. That is
the idea behind pipelining.</font> </p>

<p><strong><font face="Arial" size="3"><a NAME="HEADING5-4"></a>3.3.12.1 The 8486 Pipeline</font></strong></p>

<p><font face="Arial" size="2">Consider the steps necessary to do a generic operation: </font>

<ul>
  <li><font face="Arial" size="2">Fetch opcode. </font></li>
  <li><font face="Arial" size="2">Decode opcode and (in parallel) prefetch a possible 16-bit
    operand. </font></li>
  <li><font face="Arial" size="2">Compute complex addressing mode (e.g., <code>[xxxx+bx]</code>),
    if applicable. </font></li>
  <li><font face="Arial" size="2">Fetch the source value from memory (if a memory operand) and
    the destination register value (if applicable). </font></li>
  <li><font face="Arial" size="2">Compute the result. </font></li>
  <li><font face="Arial" size="2">Store result into destination register. </font></li>
</ul>

<p><font face="Arial" size="2">Assuming you're willing to pay for some extra silicon, you
can build a little &quot;mini-processor&quot; to handle each of the above steps. The
organization would look something likethis:</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a23.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a23.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="313" HEIGHT="81"> </font></p>

<p><font face="Arial" size="2">If you design a separate piece of hardware for each stage
in the pipeline above, almost <i>all </i>these steps can take place in parallel. Of
course, you cannot fetch and decode the opcode for any one instruction at the same time,
but you can fetch one opcode while decoding the previous instruction. If you have an
n-stage pipeline, you will usually have n instructions executing concurrently. The 8486
processor has a six stage pipeline, so it overlaps the execution of six separate
instructions. </font></p>

<p><font face="Arial" size="2">The figure below, Instruction Execution in a Pipeline,
demonstrates pipelining. T1, T2, T3, etc., represent consecutive &quot;ticks&quot; of the
system clock. At T=T1 the CPU fetches the opcode byte for the first instruction.</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a24.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a24.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="422" HEIGHT="88"> </font></p>

<p><font face="Arial" size="2">At T=T2, the CPU begins decoding the opcode for the first
instruction. In parallel, it fetches 16-bits from the prefetch queue in the event the
instruction has an operand. Since the first instruction no longer needs the opcode
fetching circuitry, the CPU instructs it to fetch the opcode of the second instruction in
parallel with the decoding of the first instruction. Note there is a minor conflict here.
The CPU is attempting to fetch the next byte from the prefetch queue for use as an
operand, at the same time it is fetching 16 bits from the prefetch queue for use as an
opcode. How can it do both at once? You'll see the solution in a few moments.</font></p>

<p><font face="Arial" size="2">At T=T3 the CPU computes an operand address for the first
instruction, if any. The CPU does nothing on the first instruction if it does not use the<code>
[xxxx+bx]</code> addressing mode. During T3, the CPU also decodes the opcode of the second
instruction and fetches any necessary operand. Finally the CPU also fetches the opcode for
the third instruction. With each advancing tick of the clock, another step in the
execution of each instruction in the pipeline completes, and the CPU fetches yet another
instruction from memory.</font></p>

<p><font face="Arial" size="2">At T=T6 the CPU completes the execution of the first
instruction, computes the result for the second, etc., and, finally, fetches the opcode
for the sixth instruction in the pipeline. The important thing to see is that after T=T5
the CPU completes an instruction on every clock cycle. <i>Once the CPU fills the pipeline,
it completes one instruction on each cycle</i>. Note that this is true even if there are
complex addressing modes to be computed, memory operands to fetch, or other operations
which use cycles on a non-pipelined processor. All you need to do is add more stages to
the pipeline, and you can still effectively process each instruction in one clock cycle.</font></p>

<p><strong><font face="Arial" size="3"><a NAME="HEADING5-23"></a>3.3.12.2 Stalls in a
Pipeline</font></strong></p>

<p><font face="Arial" size="2">Unfortunately, the scenario presented in the previous
section is a little too simplistic. There are two drawbacks to that simple pipeline: bus
contention among instructions and non-sequential program execution. Both problems may
increase the average execution time of the instructions in the pipeline.</font></p>

<p><font face="Arial" size="2">Bus contention occurs whenever an instruction needs to
access some item in memory. For example, if a <code>mov mem, reg </code>instruction needs
to store data in memory and a <code>mov reg, mem</code> instruction is reading data from
memory, contention for the address and data bus may develop since the CPU will be trying
to simultaneously fetch data and write data in memory.</font></p>

<p><font face="Arial" size="2">One simplistic way to handle bus contention is through a <i>pipeline
stall</i>. The CPU, when faced with contention for the bus, gives priority to the
instruction furthest along in the pipeline. The CPU suspends fetching opcodes until the
current instruction fetches (or stores) its operand. This causes the new instruction in
the pipeline to take two cycles to execute rather than one:</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a25.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a25.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="375" HEIGHT="131"> </font></p>

<p><font face="Arial" size="2">This example is but one case of bus contention. There are
many others. For example, as noted earlier, fetching instruction operands requires access
to the prefetch queue at the same time the CPU needs to fetch an opcode. Furthermore, on
processors a little more advanced than the 8486 (e.g., the 80486) there are other sources
of bus contention popping up as well. Given the simple scheme above, it's unlikely that
most instructions would execute at one clock per instruction (CPI).</font></p>

<p><font face="Arial" size="2">Fortunately, the intelligent use of a cache system can
eliminate many pipeline stalls like the ones discussed above. The next section on caching
will describe how this is done. However, it is not always possible, even with a cache, to
avoid stalling the pipeline. What you cannot fix in hardware, you can take care of with
software. If you avoid using memory, you can reduce bus contention and your programs will
execute faster. Likewise, using shorter instructions also reduces bus contention and the
possibility of a pipeline stall.</font></p>

<p><font face="Arial" size="2">What happens when an instruction <i>modifies</i> the <code>ip</code>
register? By the time the instruction </font></p>

<pre><font face="Courier New" size="2">		jmp	1000</font></pre>

<p><font face="Arial" size="2">completes execution, we've already started five other
instructions and we're only one clock cycle away from the completion of the first of
these. Obviously, the CPU must not execute those instructions or it will compute improper
results.</font></p>

<p><font face="Arial" size="2">The only reasonable solution is to <i>flush</i> the entire
pipeline and begin fetching opcodes anew. However, doing so causes a severe execution time
penalty. It will take six clock cycles (the length of the 8486 pipeline) before the next
instruction completes execution. Clearly, you should avoid the use of instructions which
interrupt the sequential execution of a program. This also shows another problem -
pipeline length. The longer the pipeline is, the more you can accomplish per cycle in the
system. However, lengthening a pipeline may slow a program if it jumps around quite a bit.
Unfortunately, you cannot control the number of stages in the pipeline. You can, however,
control the number of transfer instructions which appear in your programs. Obviously you
should keep these to a minimum in a pipelined system.</font></p>

<p><strong><font face="Arial" size="3"><a NAME="HEADING5-36"></a>3.3.12.3 Cache, the
Prefetch Queue, and the 8486</font></strong></p>

<p><font face="Arial" size="2">System designers can resolve many problems with bus
contention through the intelligent use of the prefetch queue and the cache memory
subsystem. They can design the prefetch queue to buffer up data from the instruction
stream, and they can design the cache with separate data and code areas. Both techniques
can improve system performance by eliminating some conflicts for the bus.</font></p>

<p><font face="Arial" size="2">The prefetch queue simply acts as a buffer between the
instruction stream in memory and the opcode fetching circuitry. Unfortunately, the
prefetch queue on the 8486 does not enjoy the advantage it had on the 8286. The prefetch
queue works well for the 8286 because the CPU isn't constantly accessing memory. When the
CPU isn't accessing memory, the BIU can fetch additional instruction opcodes for the
prefetch queue. Alas, the 8486 CPU is constantly accessing memory since it fetches an
opcode byte on every clock cycle. Therefore, the prefetch queue cannot take advantage of
any &quot;dead&quot; bus cycles to fetch additional opcode bytes - there aren't any
&quot;dead&quot; bus cycles. However, the prefetch queue is still valuable on the 8486 for
a very simple reason: the BIU fetches two bytes on each memory access, yet some
instructions are only one byte long. Without the prefetch queue, the system would have to
explicitly fetch each opcode, even if the BIU had already &quot;accidentally&quot; fetched
the opcode along with the previous instruction. With the prefetch queue, however, the
system will not refetch any opcodes. It fetches them once and saves them for use by the
opcode fetch unit.</font></p>

<p><font face="Arial" size="2">For example, if you execute two one-byte instructions in a
row, the BIU can fetch both opcodes in one memory cycle, freeing up the bus for other
operations. The CPU can use these available bus cycles to fetch additional opcodes or to
deal with other memory accesses.</font></p>

<p><font face="Arial" size="2">Of course, not all instructions are one byte long. The 8486
has two instruction sizes: one byte and three bytes. If you execute several three-byte
load instructions in a row, you're going to run slower, e.g., </font></p>

<pre><font face="Courier New" size="2">                mov     ax, 1000
                mov     bx, 2000
                mov     cx, 3000
                add     ax, 5000</font></pre>

<p><font face="Arial" size="2">Each of these instructions reads an opcode byte and a 16
bit operand (the constant). Therefore, it takes an average of 1.5 clock cycles to read
each instruction above. As a result, the instructions will require six clock cycles to
execute rather than four.</font></p>

<p><font face="Arial" size="2">Once again we return to that same rule: <i>the fastest
programs are the ones which use the shortest instructions</i>. If you can use shorter
instructions to accomplish some task, do so. The following instruction sequence provides a
good example: </font></p>

<pre><font face="Courier New" size="2">                mov     ax, 1000
                mov     bx, 1000
                mov     cx, 1000
                add     ax, 1000</font></pre>

<p><font face="Arial" size="2">We can reduce the size of this program and increase its
execution speed by changing it to: </font></p>

<pre><font face="Courier New" size="2">                mov     ax, 1000
                mov     bx, ax
                mov     cx, ax
                add     ax, ax</font></pre>

<p><font face="Arial" size="2">This code is only five bytes long compared to 12 bytes for
the previous example. The previous code will take a minimum of five clock cycles to
execute, more if there are other bus contention problems. The latter example takes only
four. Furthermore, the second example leaves the bus free for three of those four clock
periods, so the BIU can load additional opcodes. Remember, <i>shorter</i> often means <i>faster</i>.
</font></p>

<p><font face="Arial" size="2">While the prefetch queue can free up bus cycles and
eliminate bus contention, some problems still exist. Suppose the average instruction
length for a sequence of instructions is 2.5 bytes (achieved by having three three-byte
instructions and one one-byte instruction together). In such a case the bus will be kept
busy fetching opcodes and instruction operands. There will be no free time left to access
memory. Assuming some of those instructions access memory the pipeline will stall, slowing
execution.</font></p>

<p><font face="Arial" size="2">Suppose, for a moment, that the CPU has two separate memory
spaces, one for instructions and one for data, each with their own bus. This is called the
Harvard Architecture since the first such machine was built at Harvard. On a Harvard
machine there would be no contention for the bus. The BIU could continue to fetch opcodes
on the instruction bus while accessing memory on the data/memory bus:</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a26.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a26.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="354" HEIGHT="222"> </font></p>

<p><font face="Arial" size="2">In the real world, there are very few true Harvard
machines. The extra pins needed on the processor to support two physically separate busses
increase the cost of the processor and introduce many other engineering problems. However,
microprocessor designers have discovered that they can obtain many benefits of the Harvard
architecture with few of the disadvantages by using separate on-chip caches for data and
instructions. Advanced CPUs use an internal Harvard architecture and an external Von
Neumann architecture.The figure below shows the structure of the 8486 with separate data
and instruction caches.</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a27.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a27.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="251" HEIGHT="223"> </font></p>

<p><font face="Arial" size="2">Each path inside the CPU represents an independent bus.
Data can flow on all paths concurrently. This means that the prefetch queue can be pulling
instruction opcodes from the instruction cache while the execution unit is writing data to
the data cache. Now the BIU only fetches opcodes from memory whenever it cannot locate
them in the instruction cache. Likewise, the data cache buffers memory. The CPU uses the
data/address bus only when reading a value which is not in the cache or when flushing data
back to main memory.</font></p>

<p><font face="Arial" size="2">By the way, the 8486 handles the instruction operand /
opcode fetch contention problem in a sneaky fashion. By adding an extra decoder circuit,
it decodes the instruction at the beginning of the prefetch queue and three bytes into the
prefetch queue in parallel. Then, if the previous instruction did not have a 16-bit
operand, the CPU uses the result from the first decoder; if the previous instruction uses
the operand, the CPU uses the result from the second decoder.</font></p>

<p><font face="Arial" size="2">Although you cannot control the presence, size, or type of
cache on a CPU, as an assembly language programmer you must be aware of how the cache
operates to write the best programs. On-chip instruction caches are generally quite small
(8,192 bytes on the 80486, for example). Therefore, the shorter your instructions, the
more of them will fit in the cache (getting tired of &quot;shorter instructions&quot;
yet?). The more instructions you have in the cache, the less often bus contention will
occur. Likewise, using registers to hold temporary results places less strain on the data
cache so it doesn't need to flush data to memory or retrieve data from memory quite so
often. <i>Use the registers wherever possible!</i></font></p>

<p><strong><font face="Arial" size="3"><a NAME="HEADING5-70"></a>3.3.12.4 Hazards on the
8486</font></strong></p>

<p><font face="Arial" size="2">There is another problem with using a pipeline: the data
hazard. Let's look at the execution profile for the following instruction sequence: </font></p>

<pre><font face="Courier New" size="2">                mov     bx, [1000]
                mov     ax, [bx]</font></pre>

<p><font face="Arial" size="2">When these two instructions execute, the pipeline will look
something like</font></p>

<p align="center"><img SRC="images/ch03a28.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a28.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="400" HEIGHT="86"> </p>

<p><font face="Arial" size="2">Note a major problem here. These two instructions fetch the
16 bit value whose address appears at location 1000 in memory. <i>But this sequence of
instructions won't work properly!</i> Unfortunately, the second instruction has already
used the value in <code>bx</code> before the first instruction loads the contents of
memory location 1000 (T4 &amp; T6 in the diagram above).</font></p>

<p><font face="Arial" size="2">CISC processors, like the 80x86, handle hazards
automatically. However, they will stall the pipeline to synchronize the two instructions.
The actual execution on the 8486 would look something like shown below:</font></p>

<p align="center"><img SRC="images/ch03a29.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a29.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="400" HEIGHT="86"> </p>

<p><font face="Arial" size="2">By delaying the second instruction two clock cycles, the
8486 guarantees that the load instruction will load <code>ax</code> from the proper
address. Unfortunately, the second load instruction now executes in three clock cycles
rather than one. However, requiring two extra clock cycles is better than producing
incorrect results. Fortunately, you can reduce the impact of hazards on execution speed
within your software.</font></p>

<p><font face="Arial" size="2">Note that the data hazard occurs when the source operand of
one instruction was a destination operand of a previous instruction. There is nothing
wrong with loading <code>bx</code> from [1000] and then loading <code>ax</code> from<code>
[bx]</code>, <i>unless they occur one right after the other.</i> Suppose the code sequence
had been: </font></p>

<pre><font face="Courier New" size="2">                mov     cx, 2000
                mov     bx, [1000]
                mov     ax, [bx]</font></pre>

<p><font face="Arial" size="2">We could reduce the effect of the hazard that exists in
this code sequence by simply<i> rearranging the instructions.</i> Let's do that and obtain
the following: </font></p>

<pre><font face="Courier New" size="2">                mov     bx, [1000]
                mov     cx, 2000
                mov     ax, [bx]</font></pre>

<p><font face="Arial" size="2">Now the <code>mov ax</code> instruction requires only one
additional clock cycle rather than two. By inserting yet another instruction between the <code>mov
bx</code> and <code>mov ax </code>instructions you can eliminate the effects of the hazard
altogether.</font></p>

<p><font face="Arial" size="2">On a pipelined processor, the order of instructions in a
program may dramatically affect the performance of that program. Always look for possible
hazards in your instruction sequences. Eliminate them wherever possible by rearranging the
instructions. </font></p>

<p><strong><font face="Arial" size="3"><a NAME="HEADING5-95"></a>3.3.13 The 8686 Processor</font></strong></p>

<p><font face="Arial" size="2">With the pipelined architecture of the 8486 we could
achieve, at best, execution times of one CPI (clock per instruction). Is it possible to
execute instructions faster than this? At first glance you might think, &quot;Of course
not, we can do at most one operation per clock cycle. So there is no way we can execute
more than one instruction per clock cycle.&quot; Keep in mind however, that a single
instruction is <i>not </i>a single operation. In the examples presented earlier each
instruction has taken between six and eight operations to complete. By adding seven or
eight separate units to the CPU, we could effectively execute these eight operations in
one clock cycle, yielding one CPI. If we add more hardware and execute, say, 16 operations
at once, can we achieve 0.5 CPI? The answer is a qualified &quot;yes.&quot; A CPU
including this additional hardware is a <i>superscalar </i>CPU and can execute more than
one instruction during a single clock cycle. That's the capability that the 8686 processor
adds.</font></p>

<p><font face="Arial" size="2">A superscalar CPU has, essentially, several execution
units. If it encounters two or more instructions in the instruction stream (i.e., the
prefetch queue) which can execute independently, it will do so.</font></p>

<p align="center"><font face="Arial" size="2"><img SRC="images/ch03a30.gif" tppabs="http://webster.cs.ucr.edu/asm/ArtofAssembly/CH03/ch03a30.gif" NATURALSIZEFLAG="3" ALIGN="bottom" WIDTH="308" HEIGHT="223"> </font></p>

<p><font face="Arial" size="2">There are a couple of advantages to going superscalar.
Suppose you have the following instructions in the instruction stream: </font></p>

<pre><font face="Courier New" size="2">                mov     ax, 1000
                mov     bx, 2000</font></pre>

<p><font face="Arial" size="2">If there are no other problems or hazards in the
surrounding code, and all six bytes for these two instructions are currently in the
prefetch queue, there is no reason why the CPU cannot fetch and execute both instructions
in parallel. All it takes is extra silicon on the CPU chip to implement two execution
units. </font></p>

<p><font face="Arial" size="2">Besides speeding up independent instructions, a superscalar
CPU can also speed up program sequences which have hazards. One limitation of the 8486 CPU
is that once a hazard occurs, the offending instruction will completely stall the
pipeline. Every instruction which follows will also have to wait for the CPU to
synchronize the execution of the instructions. With a superscalar CPU, however,
instructions following the hazard may continue execution through the pipeline as long as
they don't have hazards of their own. This alleviates (though does not eliminate) some of
the need for careful instruction scheduling.</font></p>

<p><font face="Arial" size="2">As an assembly language programmer, the way you write
software for a superscalar CPU can dramatically affect its performance. First and foremost
is that rule you're probably sick of by now: <i>use short instructions</i>. The shorter
your instructions are, the more instructions the CPU can fetch in a single operation and,
therefore, the more likely the CPU will execute faster than one CPI. Most superscalar CPUs
do not completely duplicate the execution unit. There might be multiple ALUs, floating
point units, etc. This means that certain instruction sequences can execute very quickly
while others won't. You have to study the exact composition of your CPU to decide which
instruction sequences produce the best performance. </font></p>
<div align="center"><center>

<table border="0" width="100%" cellspacing="0" cellpadding="0">
  <tr>
    <td width="100%" valign="middle" align="center" nowrap bgcolor="#000000" height="1" colspan="3"></td>
  </tr>
  <tr>
    <td width="34%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><p align="left"><a href="CH03-4.html"><img src="../images/WB00823_.GIF" align="absmiddle" border="0" WIDTH="12" HEIGHT="24"></a><font face="Arial" size="2"><strong> <a href="CH03-4.html">Chapter Three</a> (Part 4)</strong></font></td>
    <td width="33%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><a href="../toc.html"><font face="Arial" size="2"><strong>Table of Content</strong></font></a></td>
    <td width="33%" valign="middle" align="center" bgcolor="#FFFFFF" nowrap><p align="right"><font face="Arial" size="2"><strong><a href="CH03-6.html">Chapter Three</a> (Part 6) </strong></font><a href="CH03-6.html"><img src="../images/WB00827_.GIF" align="absmiddle" border="0" WIDTH="12" HEIGHT="24"></a></td>
  </tr>
</table>
</center></div>

<p align="right"><font face="Arial" size="2"><strong>Chapter Three: System Organization
(Part 5)<br>
26 SEP 1996</strong></font></p>
</body>

<!-- Mirrored from www.arl.wustl.edu/~lockwood/class/cs306/books/artofasm/Chapter_3/CH03-5.html by HTTrack Website Copier/3.x [XR&CO'2008], Fri, 05 Dec 2008 15:25:27 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8"><!-- /Added by HTTrack -->
</html>
